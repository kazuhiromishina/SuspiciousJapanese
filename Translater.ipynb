{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chainer\n",
    "from chainer import Chain\n",
    "from chainer import Variable\n",
    "from chainer import functions as F\n",
    "from chainer import links as L\n",
    "from chainer import cuda\n",
    "from chainer import optimizers\n",
    "\n",
    "import random\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "nlp = spacy.load('ja_ginza_nopn')\n",
    "\n",
    "GPU = False\n",
    "\n",
    "if GPU:\n",
    "    chainer.cuda.get_device(0).use()\n",
    "    xp = chainer.cuda.cupy\n",
    "else:\n",
    "    xp = np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CopyNetEncoder(Chain):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super(CopyNetEncoder, self).__init__(\n",
    "            xh = L.EmbedID(vocab_size, embed_size, ignore_label=-1),\n",
    "            hh = L.Linear(embed_size, 4 * hidden_size),\n",
    "            hy = L.Linear(hidden_size, 4 * hidden_size)\n",
    "        )\n",
    "        \n",
    "    def __call__(self, x, c, h):\n",
    "        e = F.tanh(self.xh(x))\n",
    "        return F.lstm(c, self.hh(e) + self.hy(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CopyNetDecoder(Chain):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super(CopyNetDecoder, self).__init__(\n",
    "            ye = L.EmbedID(vocab_size, embed_size, ignore_label=-1),\n",
    "            eh = L.Linear(embed_size,  4*hidden_size),\n",
    "            hh = L.Linear(hidden_size, 4*hidden_size),\n",
    "            fh = L.Linear(hidden_size, 4*hidden_size),\n",
    "            bh = L.Linear(hidden_size, 4*hidden_size),\n",
    "            he = L.Linear(hidden_size, embed_size),\n",
    "            ey = L.Linear(embed_size, vocab_size)\n",
    "        )\n",
    "        \n",
    "    def __call__(self, y, c, h, f, b):\n",
    "        e = F.tanh(self.ye(y))\n",
    "        c, h = F.lstm(c, self.eh(e) + self.hh(h) + self.fh(f) + self.bh(b))\n",
    "        t = self.ey(F.tanh(self.he(h)))\n",
    "\n",
    "        return t, c, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CopyNetAttention(Chain):\n",
    "    def __init__(self, hidden_size, GPU):\n",
    "        super(CopyNetAttention, self).__init__(\n",
    "            eh = L.Linear(hidden_size, hidden_size),\n",
    "            hh = L.Linear(hidden_size, hidden_size),\n",
    "            hw = L.Linear(hidden_size, hidden_size),\n",
    "            he = L.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "                \n",
    "        if GPU:\n",
    "            self.xp = chainer.cuda.cupy\n",
    "        else:\n",
    "            self.np = np\n",
    "        \n",
    "    def __call__(self, fs, bs, h):\n",
    "        # Arguments\n",
    "        \n",
    "        # fs -> 順向きEncoder中間ベクトルのList\n",
    "        # bs -> 逆向きEncoder中間ベクトルのList\n",
    "        # h -> Decoderが出力したベクトルのList\n",
    "        \n",
    "        batch_size = h.data.shape[0]\n",
    "        \n",
    "        att = []\n",
    "        ws = []\n",
    "        \n",
    "        sum_w = Variable(\n",
    "            self.np.zeros((batch_size, 1), dtype='float32'))\n",
    "        \n",
    "        for f,b in zip(fs, bs):\n",
    "            w = self.he(F.tanh(self.eh(f)+self.hh(b)+self.he(h)))\n",
    "            att.append(w)\n",
    "            w = F.exp(w)\n",
    "            ws.append(w)\n",
    "            sum_w += w\n",
    "            \n",
    "        att_f = Variable(\n",
    "            self.np.zeros((batch_size, self.hidden_size), dtype='float32'))\n",
    "        \n",
    "        att_b = Variable(\n",
    "            self.np.zeros((batch_size, self.hidden_size), dtype='float32'))\n",
    "        \n",
    "        for f, b, w in zip(fs, bs, ws):\n",
    "            w /= sum_w\n",
    "            att_f += F.reshape(F.batch_matmul(f, w), (batch_size, self.hidden_size))\n",
    "            att_b += F.reshape(F.batch_matmul(f, w), (batch_size, self.hidden_size))\n",
    "            \n",
    "        att = F.concat(att, axis=1)\n",
    "        return att_f, att_b, att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CopyNet(Chain):\n",
    "    def __init__(self, vocab_size, hidden_size, embed_size, batch_size, GPU=False):\n",
    "        \n",
    "        super(CopyNet, self).__init__(\n",
    "            f_encoder = CopyNetEncoder(vocab_size, embed_size, hidden_size),\n",
    "            b_encoder = CopyNetEncoder(vocab_size, embed_size, hidden_size),\n",
    "            \n",
    "            attention = CopyNetAttention(hidden_size, GPU),\n",
    "            decoder   = CopyNetDecoder(vocab_size, embed_size, hidden_size),\n",
    "            predictor = L.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        if GPU:\n",
    "            chainer.cuda.get_device(0).use()\n",
    "            self.xp = chainer.cuda.cupy\n",
    "        else:\n",
    "            self.np = np\n",
    "            \n",
    "        self.vocab_size  = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_size  = embed_size\n",
    "        self.batch_size  = batch_size\n",
    "        \n",
    "        \n",
    "        self.fs = []\n",
    "        self.bs = []\n",
    "        \n",
    "        self.c = None\n",
    "        self.h = None\n",
    "        \n",
    "    def encode(self, fs):\n",
    "        \"\"\"Arguments\n",
    "        fs -> 入力する単語のList\n",
    "        \"\"\"\n",
    "        \n",
    "        c = Variable(self.np.zeros((self.batch_size, self.hidden_size), dtype='float32'))\n",
    "        h = Variable(self.np.zeros((self.batch_size, self.hidden_size), dtype='float32'))\n",
    "        \n",
    "        for word in fs:\n",
    "            c, h = self.f_encoder(word, c, h)\n",
    "            self.fs.append(h)\n",
    "\n",
    "        c = Variable(self.np.zeros((self.batch_size, self.hidden_size), dtype='float32'))\n",
    "        h = Variable(self.np.zeros((self.batch_size, self.hidden_size), dtype='float32'))\n",
    "        \n",
    "        for word in reversed(fs):\n",
    "            c, h = self.b_encoder(word, c, h)\n",
    "            self.bs.insert(0, h)\n",
    "            \n",
    "        self.c = Variable(self.np.zeros((self.batch_size, self.hidden_size), dtype='float32'))\n",
    "        self.h = Variable(self.np.zeros((self.batch_size, self.hidden_size), dtype='float32'))\n",
    "        \n",
    "    def decode(self, w):\n",
    "        \"\"\"Arguments\n",
    "        w -> 入力する単語のList\n",
    "        \"\"\"\n",
    "        \n",
    "        att_f, att_b, att = self.attention(self.fs, self.bs, self.h)\n",
    "        \n",
    "        t, self.c, self.h = self.decoder(w, self.c, self.h, att_f, att_b)\n",
    "        \n",
    "        return self.predictor(self.h), att, t\n",
    "    \n",
    "    def reset(self):\n",
    "        \n",
    "        self.fs = []\n",
    "        self.bs = []\n",
    "        \n",
    "        self.c = Variable(self.np.zeros((self.batch_size, self.hidden_size), dtype='float32'))\n",
    "        self.h = Variable(self.np.zeros((self.batch_size, self.hidden_size), dtype='float32'))\n",
    "        \n",
    "        self.zerograds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(enc_words, dec_words, model):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    \n",
    "        enc_words -> 単語IDのリストのBatch\n",
    "        dec_words -> listの単語IDのリストのBatch\n",
    "        model    -> model\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size = len(enc_words[0])\n",
    "    \n",
    "    enc_keys  = enc_words\n",
    "    \n",
    "    enc_words = [Variable(xp.array(row, dtype='int32')) for row in enc_words]\n",
    "    \n",
    "    model.reset()\n",
    "    model.encode(enc_words)\n",
    "    \n",
    "    loss = Variable(xp.zeros((), dtype='float32'))\n",
    "    t = Variable(xp.array([0 for _ in range(batch_size)], dtype='int32'))\n",
    "\n",
    "    for dec in dec_words:\n",
    "        lambda_, att, y = model.decode(t)\n",
    "        \n",
    "        t = Variable(xp.array(dec, dtype='int32'))\n",
    "        s    = F.log_softmax(y)\n",
    "        \n",
    "        att_s = F.log_softmax(att)\n",
    "        \n",
    "        lambda_s = F.reshape(F.sigmoid(lambda_), (batch_size,))\n",
    "        \n",
    "        Pg = Variable(xp.zeros((), dtype='float32'))\n",
    "        Pc = Variable(xp.zeros((), dtype='float32')) \n",
    "        Ep = Variable(xp.zeros((), dtype='float32'))\n",
    "        \n",
    "        Cnt = 0\n",
    "        \n",
    "        for i, word in enumerate(dec):\n",
    "            \n",
    "            if word != -1:\n",
    "                _x = F.get_item(F.get_item(s,i), word)\n",
    "                _y = F.reshape((1.0 - F.get_item(lambda_s, i)), ())\n",
    "                \n",
    "                Pg  += _x*_y\n",
    "                Cnt += 1\n",
    "                \n",
    "                if word in enc_keys[i]:\n",
    "                    _x = F.get_item(F.get_item(att_s, i), list(enc_keys[i]).index(word))\n",
    "                    _y = F.reshape(F.get_item(lambda_s, i), ())\n",
    "                    \n",
    "                    Pc += _x*_y\n",
    "                    \n",
    "                    Ep += F.log(F.get_item(lambda_s, i))\n",
    "                else:\n",
    "                    Ep += F.log(1.0 - F.get_item(lambda_s, i))\n",
    "                    \n",
    "        Pg *= (-1.0 / xp.max([1, Cnt]))\n",
    "        Pc *= (-1.0 / xp.max([1, Cnt]))\n",
    "        Ep *= (-1.0 / xp.max([1, Cnt]))\n",
    "        \n",
    "        loss += Pg + Pc + Ep\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, enc_words, batch_col_size):\n",
    "    result = [] # Seq の　リスト\n",
    "    modes  = [] # Copy Gen どちらを使ったか。\n",
    "    \n",
    "    model.reset()\n",
    "    \n",
    "    enc_keys  = enc_words\n",
    "    \n",
    "    enc_words = [Variable(xp.array(x, dtype='int32')) for x in enc_words]\n",
    "    \n",
    "    t = Variable(xp.array([0], dtype='int32'))\n",
    "\n",
    "    model.encode(enc_words)\n",
    "    \n",
    "    for i in range(batch_col_size):\n",
    "        lambda_, att, y = model.decode(t)\n",
    "        \n",
    "        lambda_ = F.sigmoid(lambda_)\n",
    "        \n",
    "        s = F.softmax(y)\n",
    "        \n",
    "        prob = lambda_.data[0][0]\n",
    "        \n",
    "        flag = xp.random.choice(2, 1, p=[1.0 - prob, prob])[0]\n",
    "        \n",
    "        if flag == 0:\n",
    "            label = s.data.argmax()\n",
    "            result.append(label)\n",
    "            \n",
    "            modes.append(\"Gen\")\n",
    "            \n",
    "            t = Variable(xp.array([label], dtype='int32'))\n",
    "        else:\n",
    "            n = F.softmax(att).data.argmax()\n",
    "            \n",
    "            label = enc_keys[n][0]\n",
    "            \n",
    "            result.append(label)\n",
    "            modes.append(\"Copy\")\n",
    "            \n",
    "            t = Variable(xp.array([label], dtype='int32'))\n",
    "            \n",
    "        if label == 0:\n",
    "            break\n",
    "            \n",
    "    return result, modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data,\n",
    "          vocab,\n",
    "          id2wd,\n",
    "          batch_col_size,\n",
    "          embed_size=300,\n",
    "          hidden_size=150,\n",
    "          batch_size=16,\n",
    "          epoch_num=60):\n",
    "    \n",
    "    \n",
    "    N = len(data[0])\n",
    "    \n",
    "    data = np.array(data)\n",
    "    \n",
    "    total_loss = 0\n",
    "    vocab_size = len(vocab)\n",
    "    \n",
    "    model = CopyNet(vocab_size, hidden_size, embed_size, batch_size)\n",
    "    \n",
    "    opt = optimizers.Adam()\n",
    "    opt.setup(model)\n",
    "    opt.add_hook(chainer.optimizer.GradientClipping(5))\n",
    "\n",
    "    for epoch in tqdm(range(0, epoch_num)):\n",
    "        \n",
    "        total_loss = 0\n",
    "\n",
    "        perm = np.random.permutation(N)\n",
    "        \n",
    "        for i in range(0, N, batch_size):\n",
    "            \n",
    "            mini_batch_x = data[0][perm[i:i+batch_size]]\n",
    "            mini_batch_y = data[1][perm[i:i+batch_size]]\n",
    "            \n",
    "            enc = xp.array(mini_batch_x).T\n",
    "            dec = xp.array(mini_batch_y).T\n",
    "            \n",
    "            loss = forward(enc, dec, model)\n",
    "            \n",
    "            loss.backward()\n",
    "            loss.unchain_backward()\n",
    "            total_loss += loss.data\n",
    "            opt.update()\n",
    "            \n",
    "        if (epoch+1)%10 == 0:\n",
    "            print(str(epoch) + \"Epoch | total_loss : \" + str(total_loss))\n",
    "            total_loss = 0\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    [\n",
    "        \"私は怪しい日本語翻訳機を発明した。\",\n",
    "        \"仕事が大変な時はしっかり休んで体のストレスを溜めないようにする。\",\n",
    "        \"ガキが。。。舐めてると潰すぞ。\",\n",
    "        \"あなた、私のフォロワーが18億人を突破しました！！ありがとう！！\",\n",
    "        \"イトーヨーカドー\",\n",
    "        \"Twitterデビュー！\",\n",
    "        \"Huawei\",\n",
    "        \"あなた、私は感謝するが、私は負けない！\",\n",
    "        \"中国国民党\",\n",
    "        \"北方領土を不正に占領している国はどの国でしょう？\"\n",
    "    ],\n",
    "    [\n",
    "        \"贵樣！我は正レい日本語翻译机を发明した！\",\n",
    "        \"贵樣！ 仕事か大變の时はレつかリ休んて身體の疲ねどヌトレヌを贮めないよラすゐ。\",\n",
    "        \"カギが・・・舐ぬてゐと溃ずそ\",\n",
    "        \"贵樣！私のフ口ワ一か18億人を突破レだ！！あリかどラ！！\",\n",
    "        \"亻卜一彐一力卜一\",\n",
    "        \"微博デ匕ュ一！\",\n",
    "        \"华为\",\n",
    "        \"贵樣！感谢ずゑが、わたレは负げない！\",\n",
    "        \"中国共产党\",\n",
    "        \"北方领土を不法に佔拠レでいゑのはとの國でレょラ？\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(seq):\n",
    "    \n",
    "    tokens = []\n",
    "    \n",
    "    for sent in nlp(seq).sents:\n",
    "        \n",
    "        for token in sent:\n",
    "            \n",
    "            tokens.append(token.orth_)\n",
    "            \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab_dict(x, y):\n",
    "    \n",
    "    vocab = {\"<eos>\":0, \"<unk>\":1}\n",
    "    \n",
    "    for ls in [x,y]:\n",
    "        for seq in ls:\n",
    "            for token in tokenize(seq):\n",
    "                if not token in vocab.keys():\n",
    "                    vocab[token] = len(vocab)\n",
    "                    \n",
    "    return vocab, {value:key for key, value in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2id(vocab, seq):\n",
    "    result = []\n",
    "    \n",
    "    for token in tokenize(seq):\n",
    "        \n",
    "        if token in vocab.keys():\n",
    "            \n",
    "            result.append(vocab[token])\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            result.append(1)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2train(vocab, seq, batch_col_size):\n",
    "    seqs = seq2id(vocab, seq)\n",
    "    seqs.append(0)\n",
    "        \n",
    "    x = batch_col_size - len(seqs)\n",
    "\n",
    "    while x >= 1:\n",
    "        seqs.append(-1)\n",
    "        x-=1\n",
    "        \n",
    "    return seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id2seq(id2wd, seq):\n",
    "    result = []\n",
    "    \n",
    "    for index in seq:\n",
    "        if index in id2wd.keys():\n",
    "            result.append(id2wd[index])\n",
    "        elif index == -1:\n",
    "            pass\n",
    "        else:\n",
    "            result.append(\"<unk>\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_data(data, vocab, batch_col_size):\n",
    "    \n",
    "    x, y = [], []\n",
    "    \n",
    "    for train_data in data[0]:\n",
    "        x.append(seq2train(vocab, train_data, batch_col_size))\n",
    "        \n",
    "    for train_data in data[1]:\n",
    "        y.append(seq2train(vocab, train_data, batch_col_size))\n",
    "        \n",
    "    return [x,y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, id2wd = create_vocab_dict(data[0], data[1])\n",
    "\n",
    "train_data   = make_train_data(data, vocab, 40)\n",
    "\n",
    "model        = train(train_data,\n",
    "                      vocab,\n",
    "                      id2wd,\n",
    "                      40,\n",
    "                      embed_size=300, \n",
    "                      hidden_size=150,\n",
    "                      batch_size=5,\n",
    "                      epoch_num=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = seq2train(vocab, \"私は怪しい日本語翻訳機を発明した。\",40)\n",
    "res, mode = predict(model, xp.array([seq]).T, 40)\n",
    "\"\".join(id2seq(id2wd, res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
